
## GNN在推荐上的价值
推荐系统中大部分的信息具有图结构，例如社交关系、知识图谱、user-item交互组成的二部图(bipartite graph)、序列中的item转移图。

在推荐系统中，实体间的关系有用户与用户、用户与物品、物品与物品之间关系，传统的推荐方法主要关注用户与物品间的关系，很少关注用户相互间及物品相互间的关系。图神经网络GNN的发展，为人们进一步分析推荐系统实体及其相互间的关系提供了更好的表示方式

GNN能够通过迭代传播捕捉高阶的交互，并且能够有效地整合社交关系和知识图谱等边信息(辅助信息)
传统的神经网络很难学习到数据中的高阶结构信息，而图神经网络GNN采用消息传递机制整合邻居信息，通过多层堆叠使得节点可以访问高阶邻居的信息。因此图神经网络模型近年来被广泛应用在推荐系统中，并成为最先进的方法。

在推荐系统中使用GNN的动机有两点：
1）RS中大多数据具有图结构；
2）GNN擅长捕捉节点间的连接和图数据的表示学习。


## GNN

### 简介
可以建模邻居顺序与数量不定的非欧式数据

图神经网络是利用一定的方法对节点进行描述，并经过不断的节点状态更新，得到具有包含邻居节点信息和图形拓扑结构特点的状态，最终将这些节点通过特定方法进行输出，得到需要的结果。

用途：结点分类、链接预测、图分类


### 优势
* VS MLP/CNN/RNN
  图数据中结点邻居具有两个特点，一是数量不定，二是顺序不定，因此MLP/CNN/RNN无法直接处理这样的非欧式数据而只能用GNN建模。实际上，我们可以将GNN看做一种更加泛化的模型，例如，RNN相当于线性图上的GNN，而Transformer相当于完全图上的GNN。
* VS Graph Embedding
* GNN VS Feature Concat & Collaborative Filtering & Proximity Loss

### 谱图卷积

### 消息传递

### Common GNN models
* Spectral CNN
  利用拉普拉斯矩阵定义图上的卷积算子
* ChebNet
  利用切比雪夫多项式近似，降低了计算量和参数量
* GCN
  进一步简介ChebNet，将谱图卷积和消息传递联系起来，GNN开始转向层级深度学习网络结构
* SGC
  解耦了消息传递和特征变换，简化了GCN的同时不降低效果
* GAT
  将Attention机制引入GCN中，建模了邻居结点的重要性差异，增强了GCN的表达能力
* GraphSage
  一方面，将消息传递框架范式化，分为Aggregate(聚合邻居)和Concat(融合自身)两个步骤；
  另一方面，并提出了一种简单有效的邻居采样方法，可以在大图上进行Mini-Batch训练，并且当有新的结点加入时，不需要在全图上聚合邻居，也不需要重新训练模型，可以用训练好的模型直接推断。
* PPNP
  同样采用消息传递和特征变换分离的结构，并基于个性化PageRank改进消息传递，使其可以平衡局部和全局信息
* RGCN
  对于不同类型的边对应的邻居结点采用不同的参数矩阵从而建模边的异构性，考虑了不同类型的边关系的影响，并用GCN进行建模。当边的类型很多时，参数也会变得很多，容易造成过拟合，并且不易训练，需要对参数进行规约。
* HAN
  将GAT扩展到异构图上，不仅考虑了不同邻居节点的生要性差异，也考虑了不同语义的meta path的重要性差异
* GGNN

GNN可以通过节点间的消息传播(message passing)捕捉图上的依赖。
GNN的主要思想是：迭代地聚合邻域信息，并整合聚集后的信息与当前节点的表示。
根据是否使用谱卷积算子，GNN模型可以划分为谱方法和非谱方法。
「谱方法」在傅里叶域(谱域)上定义图卷积算子，需要原始的图结构表示节点间的关系。
「非谱方法」需要设计聚合器(aggregator)和更新器(updater。聚合器用于聚合来自邻居的消息，更新器用于融合邻居节点和中心节点。

### Deeper GNN

#### 问题背景
在GCN实验中发现，一般2-3层的GCN可以取得最好的性能，继续增加层数GCN性能开始下降，达到8层以上会发生大幅度的下降

### 理论分析
不考虑层间非线性时，随着SGC层数加深，所有结点会收敛到同一个表征，结点之间变得难以分辨，这就是过平滑问题。
考虑层间非线性时，随着GCN层数加深，所有结点的表征会收敛到同一个子空间
消息传递和特征变换的耦合也是阻碍GCN加深的主要原因。


### Scalable GNN
GCN设计之初其卷积是在全图上进行的，因此GNN很多模型无法直接扩展到大图上，然而真实业务场景的图数据往往都是亿级别的。
在大图上训练GNN的方法，主要分为基于采样的方法和基于预处理的方法。

#### 基于采样的方法
主要是 Node-Wise Sampling，由 GraphSage 首次提出

#### 基于预处理的方法
基于预处理的方法是针对一类特定的GNN模型设计的，不具有通用性


### Heterogeneous GNN
结点类型和边类型多样的图称为异构图，现实场景中大多是异构图。GCN, GAT等模型无法建模异构图。
异构图模型主要有：
* RGCN
  最早，通过 Edge-Type-Specific Transformation 建模边的异构性
* HAN
  通过Node-Type-Specific Transformation建模结点的异构性，在计算Attention时不仅考虑了某Meta-Path下邻居的重要性，还考虑了不同Meta-Path之间的重要性。不过HAN比较依赖Meta-Path的人工选择。
* KGAT
  通过Edge-Type-Specific Transformation + Ralation Embedding（类似于TransR）建模结点和边的异构性。
* HGT
  在Attention计算和Message Passing阶段都考虑到了对异构性的建模，分别使用Node-Type-Specific Transformation和Edge-Type-Specific Transformation建模结点和边的异构性


## GNN推荐过程
一般说来，GNN推荐主要过程分为3个步骤：
1. 根据推荐系统实体及其相互关系构建对应的GNN模型。主要考虑如何将推荐系统中不同实体映射为GNN中图节点，不同实体间的联系映射为GNN图中对应的边，如何用的神经网络函数去拟合GNN中不同的节点。在GNN推荐模型中，不论是用户节点还是物品节点，都可以根据相关信息做嵌入化处理，得到对应节点的向量化描述。如对一个物品节点，可以根据不同用户对其评论或评分信息做嵌入化处理；对一个用户节点，可以根据其社交关系或物品购买记录做嵌入化处理。边作为反映GNN推荐模型中实体之间的联系，通常有两种处理方式：一是根据联系的类型及强弱程度不同，进行嵌入化操作；二是对边不进行任何处理，仅仅当成GNN推荐模型中，用于信息传播算法作用的媒介。
2. 决定GNN模型的信息传播与更新方法。GNN模型的变体很多，不同变体之间，信息的聚集和更新方式也不同。文献指出了在不同的GNN变体中，出现的相关聚集过程。如GCN模型，通常选择mean聚集函数；GraphSAGE模型，通常采用max函数；GIN模型，通常使用的sum函数等。不同的聚集函数适合于不同的图结构，mean函数可以反映图中节点的分布情况，max函数可以反映主要特征，sum可以考虑到比较全面的图形结构特征等。信息更新过程，主要是将信息聚集之后的结果，与中心节点进行特定的运算，并作为下一层节点的初始状态。这方面，不同的模型差异也很大，如GCN模型进行信息更新时，不会考虑中心节点的信息；GraphSAGE则会将聚集结果和中心节点的向量进行连接操作；GIN模型会将两者进行直接相加等。充分考虑推荐系统实体间关系，及对应GNN模型的结构特征，选择最合适的信息传播算法，进行信息更新，往往可以取得更好的推荐效果。
3. 提取出更新后的节点(边、子图)特征，并用相关算法实现推荐。将GNN模型完成更新之后的节点取出，作为该节点对应的实体特征，采用相关算法进行推荐。如文献[30]中，使用协同过滤推荐的方法；文献[31]中，将GNN提取的物品特征和节点特征，通过MLP进行评分预测推荐；文献[32]中，通过两种不同的神经网络，进行试验结果预测与推荐。



## 参考
[一文了解推荐系统中的图神经网络](https://cloud.tencent.com/developer/article/1871860)  
[图神经网络在推荐系统的应用研究综述](https://zhuanlan.zhihu.com/p/336583315)  
[综述|GNN在推荐系统中的挑战、方法和方向](https://zhuanlan.zhihu.com/p/421964090)  
[工业界图神经网络推荐系统综述](https://zhuanlan.zhihu.com/p/423342532)
